---
title: Clap
slug: /posts/clap
date: 2020-05-24
tags:
  - Rust
---

import { HalfWidthImage } from '../../../src/components/half_width';
import rayTracingGif from './ray-tracing.gif';

[typespecs]: https://hexdocs.pm/elixir/typespecs.html
[sppm]: https://www.ci.i.u-tokyo.ac.jp/~hachisuka/sppm.pdf
[cuda]: https://en.wikipedia.org/wiki/CUDA

Right before I first got a job, and started working mostly on the web, I was knee-deep into C++, CUDA, and the [most
buzzword-filled names](sppm) for ray tracing algorithms. Next to me, a friend was parallelizing Matrix Multiplication algorithms.

That was a world of stronly typed languages (ok let's be honest, mostly C++).
It was also a decade ago, and much has changed since then.

<HalfWidthImage src={rayTracingGif} alt="Ray Tracing" />

# What is strong/static typing?

First of all, it should be said, to get a common confusion out of the way, that static typing and strong typing are two
distinct terms

*Statically typed languages* where types for each variable must be known at compile-time, and must not be incompatible;
*Strongly typed languages* are a more subjective category, but can be described as languages that prevent you from
working around the type system's rules.

C is a statically typed languages. But it's also a weakly typed language (as contradictory as that may sound). While you
need to specify types for everything, it's very easy (and painful) to find loopholes, accidental or otherwise.

Ruby is dynamically typed. It's the kind of language where `a=1; a='a'` works just fine, because there's no specific
type bound to a variable. But it's also strongly typed, because `1 + 'a'` fails to execute.

This is confusing. I had to double check a lot of this as I was writing, because it's easy to mix up definitions. That's
why I usually prefer to avoid these terms, and just consider these as two different features that affect the overall
*type safety* of our code.

# Types in the web

The web moves fast. And it just keeps on accelerating.

Because of that speed, companies have to churn out results more quickly than software development was original geared towards. There are no longer yearly releases packaged on a CD-ROM. Code goes from my editor to your screen in a matter of hours.

That need for speed factors a lot when choosing your technology stack. C/C++ is traditionally, well, a pain in the butt. Java is cumbersome in just as many ways, if not more.
The web ended up gearing towards scripting languages such as Python and PHP, mostly because of their "scripting" ability, not so much the "language" part

Scripting languages allow you to get things done quickly, without the friction of a compilation step, having to package
a release, and the often slow feedback loops that were needed.
Just write some code, test it, and `ftp` that thing right into the server.


# Types in the web

Let's take as an example 3 of the most popular backend languages for the web: PHP, JavaScript and Ruby.

Do you know what `1 + '1'` evaluates to in each of those?

If you guessed one of them wrong (or if you felt the need to open an interpreter), you might find this frustrating too.
Here are the answers:

```bash
# JavaScript
$ node -e "console.log(1 + '1')"
11


# PHP
$ echo "<?php echo 1 + '1' ?>" | php
2

# Ruby
$ ruby -e "1 + '1'"
-e:1:in `+`: String can't be coerced into Integer (TypeError)
```

3 different languages yield 3 different results. And you can find valid arguments for either of them making sense.

- Python: `'1'` is a string, so Python assumes you want to concatenate things
- PHP: `1` is a number, and `'1'` can obviously be converted into one. `+` is a sum, so why not?
- Ruby: Wait a minute, you can't add apples and oranges!

I don't really have a strong opinion towards each argument. *(ok, I come from Ruby, and this a post on typing systems.
Just play along, please)*

# Type coercion

In fact, what happened up there in the first two languages is what's called **Implicit Type Coercion**. It's a fancy way to describe
Ruby
how a language allows values of different types to interact with each other.

In JavaScript, the addition operator, when faced with a least one `String` argument, coerces the other one to a `String`
as well. PHP seems to prefer coercing into numbers intead. It also does some very funny business if the string does not
represent a valid number. Go on, try it out, I'll wait.

Ruby, being the all-trick-pony that he is, allows each type or class to implement their own [coerce](https://apidock.com/ruby/Integer/coerce) function, so custom type implicit conversions can be implemented.

These are all nice, until you shoot yourself in the foot, and all the implicitness just turns your codebase into
a treasure hunt.


## Type coercion in static languages

C, one of the founding fathers of statically typed languages, also does type coercion, although to a very small degree.

```C
int main() {
  int x = 1;
  float y = 2.5;

  printf("%f\n", x + y); // 3.5
}
```

The addition of a regular integer with a short unsigned one works correctly, at least until an overflow can be reached.

But this is mostly it, though. Try something slighly different, like assigning a float to an integer, and your program
will compile, but produce terrible results.
C mostly takes in the bytes of those variables, and interprets them like you ask him to do so. You can cast a string to
an array of floats, but you'll do that on your own, and suffer the consequences.

# Typing systems over the years

In general, most of these behaviours are to be avoided in C, for your own safety and sanity. The language just wasn't built at a time where a compiler could be powerful enough to be more powerful. The computing power wasn't up to the task, and computer science was still a relatively new field.

And that's partly why I believe people have drifted away from typing systems over the years.

Typing systems are seens as this clunky tool that just forces you to do more work, deal with cryptic compiler errors,
and more often than not require you to explicitly convert between types. All of this while you, so sure of yourself, are
just thinking "I fucking know this is a string. But there's a number in there, and I want to sum it. I know what I'm
doing".

But no. You don't know what you're doing. None of us do.

It turns out that all these years of research and experience taught us one valuable lesson: computers are much better
than humans at finding bugs.


# Outdated 

This, in my opinion, is one of the arguments in favor of stronger typing systems. It introduces more consistency.

This isn't to say that all statically typed languages agree on how to sum integers and strings. 
If, for every new stack I need to work with, we also need to know the underlying quircks of each language, or the
individual opinions of their creators on what "makes sense", then we're setting ourselves up for disaster.

> So yes, I am in favor of more strict typing systems. And from the looks of it, a lot of the community is as well (people
> did go to the trouble of writing TypesScript and various other tools, after all).

Types are useful not just to create consistency

What then, moved us in this direction?

# 

# Types are useful restrictions

At the CPU level, everything is just 0s and 1s. We're the ones who given meaning to those.
Your computer doesn't care whether `100011` is binary for the number 13, or the ASCII code for the a `#`. But you probably do. `13 + 1`
makes a lot of sense, but `# + 1` not so much.

